### 基于 Table API 实现实时报表

Apache Flink 提供了 Table API 作为批次和流处理的统一的关系型 API（unified，relational API），即，使用相同的语义对无界的、实时的流或者有界的、批次数据集进行查询，并产生相同的结果。Flink 中的 Table API 通常用于简化数据分析、数据管道和 ETL 应用程序的定义。

#### 1、要实现的系统介绍

构建一个按照账户追踪金融交易的实时仪表盘（real-time dashboard）。管道从Kafka读取数据，将结果写到 Mysql，通过 Grafana 进行可视化。

#### 2、代码分析

代码 [flink-playgrounds](https://github.com/apache/flink-playgrounds.git) 中的 **table-walkthrough**。依赖 Java、Maven、Docker。

```java
EnvironmentSettings settings = EnvironmentSettings.inStreamingMode();
TableEnvironment tEnv = TableEnvironment.create(settings);

tEnv.executeSql("CREATE TABLE transactions (\n" +
    "    account_id  BIGINT,\n" +
    "    amount      BIGINT,\n" +
    "    transaction_time TIMESTAMP(3),\n" +
    "    WATERMARK FOR transaction_time AS transaction_time - INTERVAL '5' SECOND\n" +
    ") WITH (\n" +
    "    'connector' = 'kafka',\n" +
    "    'topic'     = 'transactions',\n" +
    "    'properties.bootstrap.servers' = 'kafka:9092',\n" +
    "    'format'    = 'csv'\n" +
    ")");

tEnv.executeSql("CREATE TABLE spend_report (\n" +
    "    account_id BIGINT,\n" +
    "    log_ts     TIMESTAMP(3),\n" +
    "    amount     BIGINT\n," +
    "    PRIMARY KEY (account_id, log_ts) NOT ENFORCED" +
    ") WITH (\n" +
    "   'connector'  = 'jdbc',\n" +
    "   'url'        = 'jdbc:mysql://mysql:3306/sql-demo',\n" +
    "   'table-name' = 'spend_report',\n" +
    "   'driver'     = 'com.mysql.jdbc.Driver',\n" +
    "   'username'   = 'sql-demo',\n" +
    "   'password'   = 'demo-sql'\n" +
    ")");

Table transactions = tEnv.from("transactions");
report(transactions).executeInsert("spend_report");

```

##### 2.1、执行环境

配置 `TableEnvironment`，表环境（table environment）是用来为 job 设置属性的，指定编写的是批处理应用还是流处理应用，并且用来创建源（sources）。walkthrough 创建了一个使用运行时流的标准表环境。

```java
EnvironmentSettings settings = EnvironmentSettings.inStreamingMode();
TableEnvironment tEnv = TableEnvironment.create(settings);
```

##### 2.2、注册表

在可以用来连接到外部系统进行读\写批和流数据的环境中注册表。表源（table source）提供了对保存在外部系统（比如，数据库、key-value存储、消息队列，或者文件系统）中数据的访问。表 sink 将表发送到外部存储系统。不同的 source 和 sink 的类型，支持不同的格式，比如 CSV、JSON、Avro 或 Parquet。

```java
tEnv.executeSql("CREATE TABLE transactions (\n" +
     "    account_id  BIGINT,\n" +
     "    amount      BIGINT,\n" +
     "    transaction_time TIMESTAMP(3),\n" +
     "    WATERMARK FOR transaction_time AS transaction_time - INTERVAL '5' SECOND\n" +
     ") WITH (\n" +
     "    'connector' = 'kafka',\n" +
     "    'topic'     = 'transactions',\n" +
     "    'properties.bootstrap.servers' = 'kafka:9092',\n" +
     "    'format'    = 'csv'\n" +
     ")");
```

注册两个表：交易输入表和花费报告输出表。从交易（`transactions`）表可以读取信用卡的交易，交易信息包含了账户ID（`account_id`），时间戳（`transaction_time`）和交易量（`amount`）。这个表是一个基于 Kafka topic 的逻辑视图，叫作 `transactions` 包含了CSV数据。

```java
tEnv.executeSql("CREATE TABLE spend_report (\n" +
    "    account_id BIGINT,\n" +
    "    log_ts     TIMESTAMP(3),\n" +
    "    amount     BIGINT\n," +
    "    PRIMARY KEY (account_id, log_ts) NOT ENFORCED" +
    ") WITH (\n" +
    "    'connector'  = 'jdbc',\n" +
    "    'url'        = 'jdbc:mysql://mysql:3306/sql-demo',\n" +
    "    'table-name' = 'spend_report',\n" +
    "    'driver'     = 'com.mysql.jdbc.Driver',\n" +
    "    'username'   = 'sql-demo',\n" +
    "    'password'   = 'demo-sql'\n" +
    ")");
```

`spend_report` 表保存聚合的最终结果。底层存储是一个 Mysql 数据库中的表。

##### 2.3、查询

配置完环境并注册了表之后，就可以构建应用，从 `TableEnvironment` 可以 `from` 输入表读取数据，并用 `executeInsert` 将结果写到输出表。`report` 函数是实现业务逻辑的地方，目前还没有实现：

```java
Table transactions = tEnv.from("transactions");
report(transactions).executeInsert("spend_report");
```

#### 3、测试

项目包含了一个测试类 `SpendReportTest`，验证报告的逻辑。它用批次模式创建了一个表环境。

```java
EnvironmentSettings settings = EnvironmentSettings.inBatchMode();
TableEnvironment tEnv = TableEnvironment.create(settings); 
```

Flink的一个特殊属性是，它提供了**跨批次和流的一致性语义**。这意味着，<u>可以基于静态数据集用批次模式开发和测试应用，并作为流应用部署到生产环境</u>。

#### 4、尝试

目前，Job 的骨架已经搭好了，可以增加一些业务逻辑了。目标是构建一个展示每天每小时每个账户总共花费的报告。这意味着时间戳字段需要被从毫秒取整为小时。

就像 SQL 查询，Flink可以筛选需要的属性并按照 keys 进行 group by，同时使用 [内置函数](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/systemFunctions.html) 比如 `floor` 和 `sum`，可以写出 `report` 函数：

```java
public static Table report(Table rows) {
    return rows.select(
            $("account_id"),
            $("transaction_time").floor(TimeIntervalUnit.HOUR).as("log_ts"),
            $("amount"))
        .groupBy($("account_id"), $("log_ts"))
        .select(
            $("account_id"),
            $("log_ts"),
            $("amount").sum().as("amount"));
}
```

#### 5、用户自定义函数

Flink 包含了有限数量的内置函数，有时需要使用[自定义函数](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/udfs.html)。比如，如果没有预定义 `floor` 函数，也可以自己实现：

```java
import java.time.LocalDateTime;
import java.time.temporal.ChronoUnit;

import org.apache.flink.table.annotation.DataTypeHint;
import org.apache.flink.table.functions.ScalarFunction;

public class MyFloor extends ScalarFunction {

    public @DataTypeHint("TIMESTAMP(3)") LocalDateTime eval(
        @DataTypeHint("TIMESTAMP(3)") LocalDateTime timestamp) {

        return timestamp.truncatedTo(ChronoUnit.HOURS);
    }
}
```

然后，就可以快速地集成到应用中。

```java
public static Table report(Table rows) {
    return rows.select(
            $("account_id"),
            call(MyFloor.class, $("transaction_time")).as("log_ts"),
            $("amount"))
        .groupBy($("account_id"), $("log_ts"))
        .select(
            $("account_id"),
            $("log_ts"),
            $("amount").sum().as("amount"));
}
```

这个查询，消费 `transactions` 表中的所有记录，计算报告，并以高效可扩展的方式输出结果。编写完 `report`，就可以运行测试类了。

#### 6、增加窗口

在数据处理中，基于时间进行分组是典型的操作，尤其是在使用无限流时。基于时间分组叫作[窗口](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/operators/windows.html)，Flink 提供了灵活的开窗（windowing）语义。最基础的窗口类型是 `Tumble` 窗口，它的大小是固定的，并且它的分桶（buckets）不重叠。

```java
public static Table report(Table rows) {
    return rows.window(Tumble.over(lit(1).hour()).on($("transaction_time")).as("log_ts"))
        .groupBy($("account_id"), $("log_ts"))
        .select(
            $("account_id"),
            $("log_ts").start().as("log_ts"),
            $("amount").sum().as("amount"));
}
```

这里对时间戳字段使用了一个小时的滚动（tumbling）窗口。所以，时间戳为 `2019-06-01 01:23:47` 的记录会被放进 `2019-06-01 01:00:00` 窗口。

基于时间的聚合是唯一的，因为在持续的流应用中，时间不同于其他属性，一般是向前的。**不同于 `floor` 和 UDF，窗口函数是固有的，允许运行时应用其它优化**。在批次上下文中，窗口提供了一个用于按照时间戳属性分组记录的方便 API。

#### 7、使用 Streaming

就是这样，一个功能完全、有状态的分布式流应用。查询持续地从 Kafka 消费交易流，计算每小时的花费，输出结果。由于输入是无边界的，查询持续运行，直到手动停止。因为，Job 使用时间窗口聚合，Flink 可以执行特殊的优化，比如在框架知道某个特定窗口不再会有新的记录时可以进行状态清理。